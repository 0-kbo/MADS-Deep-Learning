{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ff8a6b6a84b18fbf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Homework 3: Recurrent Neural Networks (100 points)\n",
    "\n",
    "### Overview\n",
    "\n",
    "We now move from image recognition to natural language processing. For this assignment, we will work with a common sentiment analysis task using the IMDB dataset. This set consists of review-label pairs, where the task is to predict whether the text of the given movie review is positive or negative, a binary classification.\n",
    "\n",
    "### RNN Architecture\n",
    "\n",
    "You will be comparing four different recurrent neural network architectures: a standard RNN, a Gated Recurrent Unit (GRU), a standard Long Short-Term Memory (LSTM), and a bidirectional LSTM. \n",
    "\n",
    "Note that a GRU/LSTM cell _is_ an RNN cell, but we will refer to an RNN in the code and questions below as the most basic RNN.\n",
    "\n",
    "### Your Task\n",
    "\n",
    "At the bottom of this notebook file, there are three short answer questions testing your understanding of this neural network architecture. \n",
    "\n",
    "Below each question is a cell with the text “Type Markdown and LaTex.” Double-click the cell and type your response to the question. Save your responses by clicking on the floppy disk icon or choosing File - Save and Checkpoint.\n",
    "\n",
    "After responding to the questions, download your notebook as a `.html` file by choosing File - Download as - html (.html). You will be submitting this `.html` file to your instructor for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.set_num_threads(4)\n",
    "torch.set_num_interop_threads(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = 'assets_week3'\n",
    "reviewVocabVectors = pickle.load(open(root_dir + '/reviewVocabVectors', 'rb'))\n",
    "trainIterator = pickle.load(open(root_dir + '/trainIterator', 'rb'))\n",
    "testIterator = pickle.load(open(root_dir + '/testIterator', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingSize = 100\n",
    "hiddenSize = 10\n",
    "dropoutRate = 0.5\n",
    "numEpochs = 5\n",
    "vocabSize = 20002\n",
    "pad = 1\n",
    "unk = 0\n",
    "\n",
    "class MyRNN(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.name = model\n",
    "        self.LSTM = (model == 'LSTM' or model == 'BiLSTM')\n",
    "        self.bidir = (model == 'BiLSTM')\n",
    "        \n",
    "        self.embed = nn.Embedding(vocabSize, embeddingSize, padding_idx = pad)\n",
    "        \n",
    "        if model == 'RNN': \n",
    "            self.rnn = nn.RNN(embeddingSize, hiddenSize)\n",
    "        elif model == 'GRU': \n",
    "            self.rnn = nn.GRU(embeddingSize, hiddenSize)\n",
    "        else: \n",
    "            self.rnn = nn.LSTM(embeddingSize, hiddenSize, bidirectional=self.bidir)\n",
    "\n",
    "        self.dense = nn.Linear(hiddenSize * (2 if self.bidir else 1), 1)\n",
    "        self.dropout = nn.Dropout(dropoutRate)\n",
    "        \n",
    "    def forward(self, text, textLengths):\n",
    "        embedded = self.dropout(self.embed(text))\n",
    "        \n",
    "        packedEmbedded = nn.utils.rnn.pack_padded_sequence(embedded, textLengths)\n",
    "        if self.LSTM: \n",
    "            packedOutput, (hidden, cell) = self.rnn(packedEmbedded)\n",
    "        else: \n",
    "            packedOutput, hidden = self.rnn(packedEmbedded)\n",
    "\n",
    "        output, outputLengths = nn.utils.rnn.pad_packed_sequence(packedOutput)\n",
    "        if self.bidir: \n",
    "            hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "        else: \n",
    "            hidden = hidden[0]\n",
    "\n",
    "        return self.dense(self.dropout(hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "basicRNN = MyRNN(model='RNN')\n",
    "GRU = MyRNN(model='GRU') # Construct a GRU model, as above\n",
    "LSTM = MyRNN(model='LSTM') # Construct a LSTM model, as above\n",
    "biLSTM = MyRNN(model='BiLSTM') # Construct a BiLSTM model, as above\n",
    "models = [basicRNN, GRU, LSTM, biLSTM]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    if model is None:\n",
    "        continue\n",
    "    model.embed.weight.data.copy_(reviewVocabVectors)\n",
    "    model.embed.weight.data[unk] = torch.zeros(embeddingSize)\n",
    "    model.embed.weight.data[pad] = torch.zeros(embeddingSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def batchAccuracy(preds, targets):\n",
    "    roundedPreds = (preds >= 0)\n",
    "    return (roundedPreds == targets).sum().item() / len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: RNN, Epoch: 1, Train Loss: 0.7023055340018114\n",
      "Model: RNN, Epoch: 2, Train Loss: 0.6921511940334154\n",
      "Model: RNN, Epoch: 3, Train Loss: 0.6874914897982117\n",
      "Model: RNN, Epoch: 4, Train Loss: 0.665387328933267\n",
      "Model: RNN, Epoch: 5, Train Loss: 0.6248484775233452\n",
      "\n",
      "Model: GRU, Epoch: 1, Train Loss: 0.6980792777922452\n",
      "Model: GRU, Epoch: 2, Train Loss: 0.6819975841075868\n",
      "Model: GRU, Epoch: 3, Train Loss: 0.6078705844062063\n",
      "Model: GRU, Epoch: 4, Train Loss: 0.4941392755112075\n",
      "Model: GRU, Epoch: 5, Train Loss: 0.40071470955448685\n",
      "\n",
      "Model: LSTM, Epoch: 1, Train Loss: 0.693562761749453\n",
      "Model: LSTM, Epoch: 2, Train Loss: 0.6738174126276275\n",
      "Model: LSTM, Epoch: 3, Train Loss: 0.5866248460529405\n",
      "Model: LSTM, Epoch: 4, Train Loss: 0.4701130262878545\n",
      "Model: LSTM, Epoch: 5, Train Loss: 0.3987897595633631\n",
      "\n",
      "Model: BiLSTM, Epoch: 1, Train Loss: 0.6933902058455036\n",
      "Model: BiLSTM, Epoch: 2, Train Loss: 0.6829730011618046\n",
      "Model: BiLSTM, Epoch: 3, Train Loss: 0.5933466122278472\n",
      "Model: BiLSTM, Epoch: 4, Train Loss: 0.4505926658735251\n",
      "Model: BiLSTM, Epoch: 5, Train Loss: 0.370823867561872\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "for model in models: \n",
    "    if model is not None:\n",
    "        model.train()\n",
    "\n",
    "for model in models:\n",
    "    if model is None:\n",
    "        continue\n",
    "        \n",
    "    torch.manual_seed(0)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    for epoch in range(numEpochs):\n",
    "        epochLoss = 0\n",
    "        for batch in trainIterator:\n",
    "            optimizer.zero_grad()\n",
    "            text, textLen = batch[0]\n",
    "            predictions = model(text, textLen).squeeze(1)\n",
    "            loss = criterion(predictions, batch[1])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epochLoss += loss.item()\n",
    "        print(f'Model: {model.name}, Epoch: {epoch + 1}, Train Loss: {epochLoss / len(trainIterator)}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: RNN, Validation Accuracy: 73.24488491048594%\n",
      "Model: GRU, Validation Accuracy: 84.69469309462916%\n",
      "Model: LSTM, Validation Accuracy: 84.59079283887469%\n",
      "Model: BiLSTM, Validation Accuracy: 84.1687979539642%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "\n",
    "for model in models: \n",
    "    if model is not None:\n",
    "        model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for model in models:\n",
    "        \n",
    "        if model is None:\n",
    "            continue\n",
    "\n",
    "        accuracy = 0.0\n",
    "        for batch in testIterator:\n",
    "            text, textLen = batch[0]\n",
    "            predictions = model(text, textLen).squeeze(1)\n",
    "            loss = criterion(predictions, batch[1])\n",
    "            acc = batchAccuracy(predictions, batch[1])\n",
    "            accuracy += acc\n",
    "        print('Model: {}, Validation Accuracy: {}%'.format(model.name, accuracy / len(testIterator) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6f36ec050380d95b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Homework Questions\n",
    "\n",
    "**To make sure your code produces consistent results, it is advisable to click \"Kernel -> Restart & Run All\" every time you want to run your code.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6a084a6888e27954",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 1: Coding (50 points)\n",
    "\n",
    "First, run the code given above to assess accuracy of the default RNN model. \n",
    "\n",
    "Next, you will need to construct three other model types (GRU, LSTM, BiLSTM) for comparison purposes. Follow the comments in box 5 to initialize the three other model types then rerun the code with all models enabled.\n",
    "\n",
    "Finally, compare the accuracies of all four models (the accuracy of the default RNN should not change from the initial run). Explain your results. In doing so, overview the advantages of the best performing architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-503b047d28162c58",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "The three other model types (GRU, LSTM, and BiLSTM) outperformed the vanilla RNN.  Of those three, the GRU performed best, with validation accuracy of 84.69%, LSTM with 84.59%, and BiLSTM with 84.17%. This increase in validation accuracy between the three added models and the vanilla model is to be expected as the added models are more complex and allow us to model long-term dependencies. The best performing architechture was the GRU model, which has fewer parameters than LSTM and is faster to train.  The efficiencies are due to GRUs having only two gates (update and reset) vs the LSTM's three gates.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3920676c328b92ee",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 2: LSTM Gates (30 points)\n",
    "\n",
    "LSTMs improve upon the naive RNN architecture by adding a series of gates instead of a simple matrix-vector computation. Name the gates and explain each of their functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-2bb845467aba5dd6",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "There are three different gates in an LSTM cell: forget gate, input gate, and output gate. The forget gate decides which information from the previous cell state should be thrown away or kept.  The input gate is used to quantify the importance of the new information, determining which parts of it to keep.  The output gate controls what part of the cell contents are passed to the next hidden state. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ab00430a80313063",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 3: Applications (20 points)\n",
    "\n",
    "LSTMs are used across many different fields, from music generation to sentiment classification to text generation. Where could they be useful in your life, whether at home, for your family, or in the workplace? Give a specific problem or application for an LSTM model that was not covered in the course slides (**though it can be related to the applications covered in the slides**). Then, with your application in mind, specifically identify the input to your model, the output from your model, and an applicable loss function. \n",
    "\n",
    "(As an optional extension, try implementing your LSTM on your own using the code framework given in this homework!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-51a908d0d4f3f225",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "I work at an elementary school and each day someone on the leadership team sends an email to all employees titled \"The Daily Rundown\" that details the day at a high level, mentioning anything exceptional that went on that day.  Most days, nothing exceptional goes on and the email (both the writing and the reading) are superfluous. I'd like to highlight how much of a waste of time the email is by training an LSTM to write it for me. For this project I would be training my model on the text from past emails (input) and outputting predicted text in the form of Daily Rundown emails.  More specifically though, I would be predicting the next character in a sequence given the all the characters computed up to that moment. I would use a categorical cross-entropy loss function for this problem to increase the probability of the correct class, and to decrease the probabilities of the rival classes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
