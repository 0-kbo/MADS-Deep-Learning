{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6a65256d7eef6e7d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Homework 2: Convolutional Neural Networks (100 points)\n",
    "\n",
    "### Overview\n",
    "\n",
    "With new knowledge of convolutional neural networks, we can accomplish a more difficult image recognition task. The CIFAR-10 classification dataset consists of 60,000 labelled images split between 10 classes: airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships and trucks.\n",
    "\n",
    "For the purposes of this assignment, we will compare two models on the same dataset: a fully connected neural network (as in Homework 1) called ANN and a new convolutional architecture called CNN, as outlined in the next section. To be fair, we attempt to allow the same number of trainable parameters in the ANN as the CNN, which means we need to use the same input transformation to flatten grayscale used in Homework 1 for the ANN. The CNN reaps the full benefit of the original 2D image in RGB.\n",
    "\n",
    "### CNN Architecture\n",
    "\n",
    "Each image consists of 32x32 RGB pixel values between 0 and 255. We do not need to perform any preprocessing as the convolutional model will use all three channels concurrently as input.\n",
    "\n",
    "The architecture in use has 5 layers: a convolution layer followed by a pooling layer, then another convolutional layer, then two fully connected dense layers. The latter of these has 10 neurons to provide classification output.\n",
    "\n",
    "### Your Task\n",
    "\n",
    "At the bottom of this notebook file, there are four short answer questions testing your understanding of this neural network architecture. As before, some questions will require you to experiment with model hyperparameters.\n",
    "\n",
    "Below each question is a cell with the text “Type Markdown and LaTex.” Double-click the cell and type your response to the question. Save your responses by clicking on the floppy disk icon or choosing File - Save and Checkpoint.\n",
    "\n",
    "After responding to the questions, download your notebook as a `.html` file by choosing File - Download as - html (.html). You will be submitting this `.html` file to your instructor for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.set_num_threads(4)\n",
    "torch.set_num_interop_threads(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainTransform = transforms.Compose([#add yours here!\n",
    "                                     transforms.RandomRotation(5),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                    ])\n",
    "testTransform = transforms.Compose([transforms.ToTensor(),\n",
    "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "root_dir = 'assets_week2'\n",
    "trainDataset = torchvision.datasets.CIFAR10(root=root_dir, train=True, download=True, transform=trainTransform)\n",
    "trainLoader = torch.utils.data.DataLoader(trainDataset, batch_size=4, shuffle=True, num_workers=2)\n",
    "testDataset = torchvision.datasets.CIFAR10(root=root_dir, train=False, download=True, transform=testTransform)\n",
    "testLoader = torch.utils.data.DataLoader(testDataset, batch_size=4, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNModel(nn.Module):\n",
    "    def __init__(self, hiddenSize, dropoutRate, activate):\n",
    "        super().__init__()\n",
    "        # Note that 'layer' and 'dense' differ only in name (to show similarity to CNN)\n",
    "        self.activate = nn.Sigmoid() if activate == \"Sigmoid\" else nn.ReLU()\n",
    "        self.layer1 = nn.Linear(1024, 100)\n",
    "        self.layer2 = nn.Linear(100, 15 * 5 * 5)\n",
    "        self.dense1 = nn.Linear(15 * 5 * 5, hiddenSize)\n",
    "        self.dropout = nn.Dropout(dropoutRate)\n",
    "        self.dense2 = nn.Linear(hiddenSize, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.activate(self.layer1(x))\n",
    "        x = self.activate(self.layer2(x))\n",
    "        x = self.dropout(self.activate(self.dense1(x)))\n",
    "        return self.dense2(x)\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, hiddenSize, outChannels, dropoutRate, activate):\n",
    "        super().__init__()\n",
    "        self.outChannels = outChannels\n",
    "        self.activate = nn.Sigmoid() if activate == \"Sigmoid\" else nn.ReLU()\n",
    "        self.conv1 = nn.Conv2d(3, 24, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(24, outChannels, 5)\n",
    "        self.dense1 = nn.Linear(outChannels * 5 * 5, hiddenSize)\n",
    "        self.dropout = nn.Dropout(dropoutRate)\n",
    "        self.dense2 = nn.Linear(hiddenSize, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.activate(self.conv1(x)))\n",
    "        x = self.pool(self.activate(self.conv2(x)))\n",
    "        x = x.view(-1, self.outChannels * 5 * 5)\n",
    "        x = self.dropout(self.activate(self.dense1(x)))\n",
    "        return self.dense2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of neurons in the first fully-connected layer\n",
    "hiddenSize = 100\n",
    "# Number of feature filters in second convolutional layer\n",
    "numFilters = 25\n",
    "# Dropout rate\n",
    "dropoutRate = 0\n",
    "# Activation function\n",
    "activation = \"ReLU\"\n",
    "# Learning rate\n",
    "learningRate = 0.001\n",
    "# Momentum for SGD optimizer\n",
    "momentum = 0.9\n",
    "# Number of training epochs\n",
    "numEpochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Beginning training!\n",
      "Epoch [1/10], Step [2000/12500], ANN Loss: 2.0863615140616893, CNN Loss: 1.9889075910151004\n",
      "Epoch [1/10], Step [4000/12500], ANN Loss: 1.9419296706914901, CNN Loss: 1.6322043964415789\n",
      "Epoch [1/10], Step [6000/12500], ANN Loss: 1.861800346046686, CNN Loss: 1.4897670658156277\n",
      "Epoch [1/10], Step [8000/12500], ANN Loss: 1.8459010553061963, CNN Loss: 1.454917872980237\n",
      "Epoch [1/10], Step [10000/12500], ANN Loss: 1.813395178437233, CNN Loss: 1.3596445694752037\n",
      "Epoch [1/10], Step [12000/12500], ANN Loss: 1.795057803362608, CNN Loss: 1.33835771510005\n",
      "Epoch [2/10], Step [2000/12500], ANN Loss: 1.7331264152526855, CNN Loss: 1.2524616960138082\n",
      "Epoch [2/10], Step [4000/12500], ANN Loss: 1.728240273565054, CNN Loss: 1.239610386964865\n",
      "Epoch [2/10], Step [6000/12500], ANN Loss: 1.7069842965602875, CNN Loss: 1.2036444773636759\n",
      "Epoch [2/10], Step [8000/12500], ANN Loss: 1.7102003564983606, CNN Loss: 1.1794171325024216\n",
      "Epoch [2/10], Step [10000/12500], ANN Loss: 1.6928471272289753, CNN Loss: 1.1510059531331063\n",
      "Epoch [2/10], Step [12000/12500], ANN Loss: 1.697136388540268, CNN Loss: 1.1497407417483627\n",
      "Epoch [3/10], Step [2000/12500], ANN Loss: 1.6246948527246714, CNN Loss: 1.0461631833203138\n",
      "Epoch [3/10], Step [4000/12500], ANN Loss: 1.6330633825063705, CNN Loss: 1.0592160558328032\n",
      "Epoch [3/10], Step [6000/12500], ANN Loss: 1.6318599126189948, CNN Loss: 1.070126178778708\n",
      "Epoch [3/10], Step [8000/12500], ANN Loss: 1.6322651762366296, CNN Loss: 1.0408527708714828\n",
      "Epoch [3/10], Step [10000/12500], ANN Loss: 1.623321356073022, CNN Loss: 1.0496272431910039\n",
      "Epoch [3/10], Step [12000/12500], ANN Loss: 1.6231977630704642, CNN Loss: 1.0103045064080507\n",
      "Epoch [4/10], Step [2000/12500], ANN Loss: 1.5586261459589004, CNN Loss: 0.9360461231181398\n",
      "Epoch [4/10], Step [4000/12500], ANN Loss: 1.5606356519982219, CNN Loss: 0.9622165821287781\n",
      "Epoch [4/10], Step [6000/12500], ANN Loss: 1.5767647240757943, CNN Loss: 0.9598218034299789\n",
      "Epoch [4/10], Step [8000/12500], ANN Loss: 1.588342294856906, CNN Loss: 0.9461795757813379\n",
      "Epoch [4/10], Step [10000/12500], ANN Loss: 1.5826593451425433, CNN Loss: 0.9561171568292193\n",
      "Epoch [4/10], Step [12000/12500], ANN Loss: 1.5723060850277544, CNN Loss: 0.9574039197657257\n",
      "Epoch [5/10], Step [2000/12500], ANN Loss: 1.5044679782539605, CNN Loss: 0.8726544031761587\n",
      "Epoch [5/10], Step [4000/12500], ANN Loss: 1.5151363275349141, CNN Loss: 0.8678722233981826\n",
      "Epoch [5/10], Step [6000/12500], ANN Loss: 1.5180190808176994, CNN Loss: 0.88597200510744\n",
      "Epoch [5/10], Step [8000/12500], ANN Loss: 1.5311081834957003, CNN Loss: 0.8854047138672322\n",
      "Epoch [5/10], Step [10000/12500], ANN Loss: 1.5500058163776993, CNN Loss: 0.8986205133583862\n",
      "Epoch [5/10], Step [12000/12500], ANN Loss: 1.540289981558919, CNN Loss: 0.8911209004065022\n",
      "Epoch [6/10], Step [2000/12500], ANN Loss: 1.4548513209819793, CNN Loss: 0.809451648616232\n",
      "Epoch [6/10], Step [4000/12500], ANN Loss: 1.4608228703439237, CNN Loss: 0.8119959960316774\n",
      "Epoch [6/10], Step [6000/12500], ANN Loss: 1.4931529849395155, CNN Loss: 0.8498049990036525\n",
      "Epoch [6/10], Step [8000/12500], ANN Loss: 1.486479825258255, CNN Loss: 0.8248619652660564\n",
      "Epoch [6/10], Step [10000/12500], ANN Loss: 1.5154222001433373, CNN Loss: 0.8568622934720479\n",
      "Epoch [6/10], Step [12000/12500], ANN Loss: 1.4937988218665124, CNN Loss: 0.8503903871093644\n",
      "Epoch [7/10], Step [2000/12500], ANN Loss: 1.397298394843936, CNN Loss: 0.736784393743379\n",
      "Epoch [7/10], Step [4000/12500], ANN Loss: 1.4518526581898332, CNN Loss: 0.7565507060805102\n",
      "Epoch [7/10], Step [6000/12500], ANN Loss: 1.4392909097149968, CNN Loss: 0.7966153030602727\n",
      "Epoch [7/10], Step [8000/12500], ANN Loss: 1.4747839274778962, CNN Loss: 0.8021954823547276\n",
      "Epoch [7/10], Step [10000/12500], ANN Loss: 1.4590956071317196, CNN Loss: 0.8269039131260942\n",
      "Epoch [7/10], Step [12000/12500], ANN Loss: 1.4601960791647435, CNN Loss: 0.8215961391353048\n",
      "Epoch [8/10], Step [2000/12500], ANN Loss: 1.3823455104492606, CNN Loss: 0.7296377797151509\n",
      "Epoch [8/10], Step [4000/12500], ANN Loss: 1.4039677172154188, CNN Loss: 0.7533844913913199\n",
      "Epoch [8/10], Step [6000/12500], ANN Loss: 1.4136860244348646, CNN Loss: 0.7641937828490045\n",
      "Epoch [8/10], Step [8000/12500], ANN Loss: 1.4259191524758934, CNN Loss: 0.7494992538355291\n",
      "Epoch [8/10], Step [10000/12500], ANN Loss: 1.4359426517114042, CNN Loss: 0.7648768347957521\n",
      "Epoch [8/10], Step [12000/12500], ANN Loss: 1.4205806144550444, CNN Loss: 0.7749379120866797\n",
      "Epoch [9/10], Step [2000/12500], ANN Loss: 1.3368834146559239, CNN Loss: 0.6671181604607264\n",
      "Epoch [9/10], Step [4000/12500], ANN Loss: 1.3573095982763916, CNN Loss: 0.7291212758244947\n",
      "Epoch [9/10], Step [6000/12500], ANN Loss: 1.3802927555348725, CNN Loss: 0.7165162108842633\n",
      "Epoch [9/10], Step [8000/12500], ANN Loss: 1.4084301419705152, CNN Loss: 0.7442694787797373\n",
      "Epoch [9/10], Step [10000/12500], ANN Loss: 1.4115345520228149, CNN Loss: 0.7445063837088819\n",
      "Epoch [9/10], Step [12000/12500], ANN Loss: 1.4256795959994197, CNN Loss: 0.7522682550332974\n",
      "Epoch [10/10], Step [2000/12500], ANN Loss: 1.302660088817589, CNN Loss: 0.6457671776905191\n",
      "Epoch [10/10], Step [4000/12500], ANN Loss: 1.3407988149523735, CNN Loss: 0.6902675026110082\n",
      "Epoch [10/10], Step [6000/12500], ANN Loss: 1.375142890661955, CNN Loss: 0.6933915172356646\n",
      "Epoch [10/10], Step [8000/12500], ANN Loss: 1.3710179367139936, CNN Loss: 0.7206170596593511\n",
      "Epoch [10/10], Step [10000/12500], ANN Loss: 1.362707790158689, CNN Loss: 0.7063077790902462\n",
      "Epoch [10/10], Step [12000/12500], ANN Loss: 1.4118024893179535, CNN Loss: 0.7346319855613401\n",
      "\n",
      ">>> Beginning validation!\n",
      "ANN validation accuracy: 43.28%, CNN validation accuracy: 67.36999999999999%\n"
     ]
    }
   ],
   "source": [
    "ann = ANNModel(hiddenSize, dropoutRate, activation)\n",
    "cnn = CNNModel(hiddenSize, numFilters, dropoutRate, activation)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(list(ann.parameters()) + list(cnn.parameters()), lr=learningRate, momentum=momentum)\n",
    "\n",
    "print('>>> Beginning training!') \n",
    "ann.train()\n",
    "cnn.train()\n",
    "for epoch in range(numEpochs):  # loop over the dataset multiple times\n",
    "    annRunningLoss, cnnRunningLoss = 0, 0\n",
    "    for i, (inputs, labels) in enumerate(trainLoader, 0):\n",
    "        annInputs = torch.sum(inputs, axis=1).view(-1, 32*32)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward propagation\n",
    "        annOutputs = ann(annInputs)\n",
    "        cnnOutputs = cnn(inputs)\n",
    "        \n",
    "        # Backpropagation\n",
    "        annLoss = criterion(annOutputs, labels)\n",
    "        cnnLoss = criterion(cnnOutputs, labels)\n",
    "        annLoss.backward()\n",
    "        cnnLoss.backward()\n",
    "        \n",
    "        # Gradient update\n",
    "        optimizer.step()\n",
    "\n",
    "        annRunningLoss += annLoss.item()\n",
    "        cnnRunningLoss += cnnLoss.item()\n",
    "        if (i+1) % 2000 == 0:    # print every 2000 mini-batches\n",
    "            print('Epoch [{}/{}], Step [{}/{}], ANN Loss: {}, CNN Loss: {}'.format(epoch + 1, numEpochs, i + 1, len(trainDataset)//4, annRunningLoss/2000, cnnRunningLoss/2000))\n",
    "            annRunningLoss, cnnRunningLoss = 0, 0\n",
    "\n",
    "print()\n",
    "print('>>> Beginning validation!')\n",
    "ann.eval()\n",
    "cnn.eval()\n",
    "annCorrect, cnnCorrect = 0, 0\n",
    "total = 0\n",
    "for inputs, labels in testLoader:\n",
    "    annInputs = torch.sum(inputs, axis=1).view(-1, 32*32)\n",
    "    annOutputs = ann(annInputs)\n",
    "    cnnOutputs = cnn(inputs)\n",
    "    _, annPredicted = torch.max(annOutputs.data, 1)\n",
    "    _, cnnPredicted = torch.max(cnnOutputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    annCorrect += (annPredicted == labels).sum().item()\n",
    "    cnnCorrect += (cnnPredicted == labels).sum().item()\n",
    "print('ANN validation accuracy: {}%, CNN validation accuracy: {}%'.format(annCorrect / total * 100, cnnCorrect / total * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework Questions\n",
    "\n",
    "**To make sure your code produces consistent results, it is advisable to click \"Kernel -> Restart & Run All\" every time you want to run your code.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9d88fe0d5a5da473",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 1: CNN Advantage (10 points)\n",
    "\n",
    "Compute the accuracy of a simple dense neural network and a simple CNN on the dataset. Explain the results and briefly overview the advantages of a CNN over a standard neural network for image-related tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-90d19f37b669e57c",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "The CNN performed better than the dense neural network (~69% compared to 42%).  CNNs perform better than standard neural networks for image-related tasks because of the sheer number of parameters that must be learned in standard neural networks and the importance of spatial structure in images. Spatial proximity is important when understanding images and CNNs maintain the important spatial structure by use of spatial filters. CNNs also benefit from reducing the dimensionality of input image data (through convolution operations) prior to feeding it to a fully connected layer(s). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ff132f3e79a9ae46",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 2: Dropout Rate (25 points)\n",
    "\n",
    "Explain the purpose of dropout in any neural network model. In doing so, note what can happen if the dropout rate is too high and what can happen if the dropout rate is too low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1d4aaf9724eff071",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Dropout modifies the model by randomly selecting activations and setting them to zero in the hidden layer (creating a simpler model) during each iteration of training.  Its purpose is to force the model not to rely on any one node while making predictions and reduce model complexity.  If the dropout rate is too high, our model's convergence rate will be too slow and its performance will suffer.  Too low of a dropout rate does not produce generalization improvements for our model and we run the risk of overfitting.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e05305cff2c7415f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 3: Kernel Size (25 points)\n",
    "\n",
    "Explain the purpose of spatial filters (kernels) in a CNN. Additionally, explain where they fit into the overall architecture of the CNN in this coding example. Finally, explain what can happen if the kernel size is too large and what can happen if the kernel size is too small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5bd1497e584129ea",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "The kernel in a CNN is a small matrix that moves over the input data, performs the dot product with a region of the input volume and gets the output as a matrix of dot products. The kernel size fits into the convolution layer; namely, above the kernel size is being set to 5 in both the first and second convolution layers (nn.Conv2d(3, 24, 5) & nn.Conv2d(24, outChannels, 5)). If the kernel size is too large, you miss out on extracting more low-level details from the image and lead to underfitting.  Conversely, if the kernel size is too small, you may miss out on more global features of the input data and run the risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-42306086a7bdf4e5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 4: Data Augmentation (40 points)\n",
    "\n",
    "Use the code snippet provided in the next box to implement data augmentation by updating the contents of box 3 and re-running the model. Compare your accuracy without and with data augmentation and explain the results. In doing so, explain the purpose of data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ef28cafc520fc2a3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "transforms.RandomRotation(5),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ea6c603cd9e5a90a",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "In Q4 we implemented data augmentation in our model.  Data augmentation is a way to generate more training data from our current set by transforming the input data (via rotaion, scaling, brightness) and adding transformed data to the training data. This method forces the model to generalize as it is unable to overfit to the diverse training data. In this case after adding data augmentation the ANN validation accuracy increased very little (42.12% to 43.28%) and the CNN validation accuracy decreased slightly (68.66% to 67.40%). This is a good reminder that just adding any amount of regularization techniques does not always lead to increased accuracy scores."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
