{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1: Feed-forward Neural Networks (100 points)\n",
    "\n",
    "### Overview\n",
    "\n",
    "Below you will find a PyTorch implementation of a feed-forward neural network for image recognition. We use the popular MNIST dataset, where the model predicts a single digit (0-9) for a black-and-white photo of a handwritten digit. This is a _classification_ task.\n",
    "\n",
    "### NN Architecture\n",
    "\n",
    "Each image has size 28x28 grayscale pixel values between 0 and 255. In preprocessing, we flatten each image to a single vector of length $28^2 = 784$, which serves as the entire input for the model.\n",
    "\n",
    "For each image, we aim to predict one of ten classes (0-9). We could use an output layer $y$ of size 1 (a single neuron) -- for example, using a naive mapping like prediction $p = \\mathrm{int}(10y)$. But this presupposes that a handwritten 0 is similar to a handwritten 1 and very different than a handwritten 9, which isn't the case. So instead we use an output layer $y$ of size 10, where the prediction $p = argmax(y)$, so each output neuron controls the likelihood for a particular class.\n",
    "\n",
    "We use a simple two-layer neural network. To begin, we will have an input size of 784, a hidden layer of size 5, and an output layer of size 10.\n",
    "\n",
    "### Your Task\n",
    "\n",
    "At the bottom of this notebook file, there are a series of questions testing your understanding of this neural network architecture. Some questions include instructions where you will need to modify hyperparameters (notated in the code below) and re-run the model to investigate the changed results. __There is no need to read through the following code in depth to answer the questions, but it may be useful as a reference.__\n",
    "\n",
    "Below each question is a cell with the text “Type Markdown and LaTex.” Double-click the cell and type your response to the question. Save your responses by clicking on the floppy disk icon or choosing File - Save and Checkpoint.\n",
    "\n",
    "After responding to the questions, download your notebook as a `.html` file by choosing File - Download as - html (.html). You will be submitting this `.html` file to your instructor for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f6294778e90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = 'assets_week1'\n",
    "trainDataset = datasets.MNIST(root=root_dir, train=True, transform=transforms.ToTensor(), download=True)\n",
    "testDataset = datasets.MNIST(root=root_dir, train=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNModel(nn.Module):\n",
    "    def __init__(self, inputSize, outputSize, hiddenSize, activate):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.activate = nn.Sigmoid() if activate == \"Sigmoid\" else nn.Tanh() if activate == \"Tanh\" else nn.ReLU()\n",
    "        self.layer1 = nn.Linear(inputSize, hiddenSize)\n",
    "        self.layer2 = nn.Linear(hiddenSize, outputSize)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        hidden = self.activate(self.layer1(X))\n",
    "        return self.layer2(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dimensionality of the input\n",
    "inputSize = 784\n",
    "# Number of neurons in the first layer\n",
    "hiddenSize = 300\n",
    "# Number of neurons in the second layer\n",
    "outputSize = 10\n",
    "# Activation function (default: ReLU, options: Sigmoid, Tanh, ReLU)\n",
    "activation = \"ReLU\"\n",
    "# Learning rate\n",
    "learningRate = .001\n",
    "# Number of training epochs\n",
    "numEpochs = 5\n",
    "# Number of training examples per batch\n",
    "batchSize = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Beginning training!\n",
      "Epoch [1/5], Step [100/300], Loss: 0.3068412244319916\n",
      "Epoch [1/5], Step [200/300], Loss: 0.28995949029922485\n",
      "Epoch [1/5], Step [300/300], Loss: 0.1639423370361328\n",
      "Epoch [2/5], Step [100/300], Loss: 0.24531342089176178\n",
      "Epoch [2/5], Step [200/300], Loss: 0.11324802041053772\n",
      "Epoch [2/5], Step [300/300], Loss: 0.08485922962427139\n",
      "Epoch [3/5], Step [100/300], Loss: 0.0982745960354805\n",
      "Epoch [3/5], Step [200/300], Loss: 0.06796352565288544\n",
      "Epoch [3/5], Step [300/300], Loss: 0.11601042747497559\n",
      "Epoch [4/5], Step [100/300], Loss: 0.06404197961091995\n",
      "Epoch [4/5], Step [200/300], Loss: 0.0819862112402916\n",
      "Epoch [4/5], Step [300/300], Loss: 0.09553879499435425\n",
      "Epoch [5/5], Step [100/300], Loss: 0.07789668440818787\n",
      "Epoch [5/5], Step [200/300], Loss: 0.06512964516878128\n",
      "Epoch [5/5], Step [300/300], Loss: 0.07752872258424759\n",
      "\n",
      ">>> Beginning validation!\n",
      "Validation accuracy: 97.55%\n"
     ]
    }
   ],
   "source": [
    "trainLoader = torch.utils.data.DataLoader(dataset=trainDataset, batch_size=batchSize, shuffle=True)\n",
    "testLoader = torch.utils.data.DataLoader(dataset=testDataset, batch_size=batchSize, shuffle=False)\n",
    "\n",
    "net = NNModel(inputSize, outputSize, hiddenSize, activation)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learningRate)\n",
    "\n",
    "print('>>> Beginning training!')\n",
    "for epoch in range(numEpochs):\n",
    "    for i, (images, labels) in enumerate(trainLoader):\n",
    "        images = images.view(-1, 28*28)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward propagation\n",
    "        outputs = net(images)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient descent\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Logging\n",
    "        if (i+1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {}'.format(epoch+1, numEpochs, i+1, len(trainDataset)//batchSize, loss))\n",
    "\n",
    "print()\n",
    "print('>>> Beginning validation!')\n",
    "correct, total = 0, 0\n",
    "for i, (images, labels) in enumerate(testLoader):\n",
    "    images = images.view(-1, 28*28)\n",
    "    \n",
    "    outputs = net(images)\n",
    "    _, prediction = torch.max(outputs, axis=1)\n",
    "    correct += torch.sum(prediction == labels)\n",
    "    total += labels.size(0)\n",
    "print('Validation accuracy: {}%'.format(correct.item()/total*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5e66e36a749e1d3e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Homework Questions\n",
    "\n",
    "Your goal is to improve the model's accuracy by tuning hyperparameters. If a question asks you to modify a hyperparameter and you obtain improved results, retain that hyperparameter change for subsequent questions. Otherwise, revert back to the original hyperparameter value.\n",
    "\n",
    "**To make sure your code produces consistent results, it is advisable to click \"Kernel -> Restart & Run All\" every time you want to run your code.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f7531bbce136967d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 1: Loss Minimization & Gradient Descent (5 points)\n",
    "\n",
    "Given a neural network with model parameters $\\theta$, loss function $E$, and learning rate $\\alpha$, what is the correct method to perform gradient descent?\n",
    "\n",
    "a) $\\theta_i += \\alpha E$\n",
    "\n",
    "b) $\\theta_i -= \\alpha E$\n",
    "\n",
    "c) $\\theta_i += \\alpha\\frac{\\partial E}{\\partial \\theta_i}$\n",
    "\n",
    "d) $\\theta_i -= \\alpha\\frac{\\partial E}{\\partial \\theta_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a78c77dc1b23ccb9",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "C. Update params in the opposite direction of the gradient of the objective function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7257d9aaf2ae5f52",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 2: Class Imbalance (10 points)\n",
    "\n",
    "Imagine you are an engineer tasked with helping a company to identify faulty parts early using an machine learning-based image recognition system. What evaluation metric would you use? More specifically, explain why a raw percent accuracy score would be a poor choice of evaluation metric for this problem space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d1ceb13ddbf02d25",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "I would suggest using Recall as an evaluation metric. To identify faulty parts early, we'd want to limit false negatives (a faulty part slipping past our detection) and therefore increase sensitivity, which Recall does. A raw percent accuracy score is a poor choice of evaluation metric for imbalanced classification because high accuracy is achievable by a model that simply only predicts the majority class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7e740ab84baa5c87",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 3a:  Size of a Hidden Layer (10 points)\n",
    "\n",
    "Explain how the hidden layer size influences the architecture of a feed-forward neural network. In doing so, note what can happen if the hidden size is too large and what can happen if the hidden size is too small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-0756abe8399f8d1b",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Hidden layer size influences the complexity of functions our feed-forward neural network will be able to fit.  The more nodes we have in our hidden layers, the more parameters we must optimize. If the hidden size is too large, computations requirements become too large and we risk overfitting our data.  If the hidden size is too small we risk underfitting our data and over generalizing our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8918fa8441b6410a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 3b: Size of a Hidden Layer  (10 points)\n",
    "\n",
    "Increase the hidden size from 5 to 300 and re-run your trial. How does the accuracy change?\n",
    "\n",
    "_a) It increases, since the model learns more quickly_\n",
    "\n",
    "_b) It increases, since the model has more memory and can learn more complex features_\n",
    "\n",
    "_c) It decreases, since the model has to learn more parameters and it doesn't have enough time_\n",
    "\n",
    "_d) It decreases, since the model has less memory_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-fa1da64d2d3807df",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "B. It increases, since the model has more memory and can learn more complex features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-df1fbf77fef73231",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 4a: Learning Rate  (10 points)\n",
    "\n",
    "Explain the purpose of a learning rate. In doing so, note what can happen if the learning rate is too large and what can happen if the learning rate is too small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-01781cd504996df3",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "The learning rate controls how big of a jump (step) we take in response to the estimated error each time we updated model weights on our way to reach a minimum. If the learning rate is too small, our model will take too long to converge and be computationally expensive.  If learning rate is too large, the steps may skip over minimums and converge on suboptimal weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e14ac17232cdd618",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 4b: Learning Rate  (10 points)\n",
    "\n",
    "Increase the learning rate from 0.001 to 1. How does the accuracy change?\n",
    "\n",
    "a) It increases, since the model learns more quickly\n",
    "\n",
    "b) It increases, since the model is better able to converge\n",
    "\n",
    "c) It decreases, since the model learns too slowly\n",
    "\n",
    "d) It decreases, since the model is not able to converge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-7fd150248c4bd476",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "D. It decreases, since the model is not able to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d7c6dda3a2679bae",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 5a: Activation Functions (10 points)\n",
    "\n",
    "Explain the main purpose of an activation function in neural networks. Also, explain the main benefit of the Tanh activation function over the Sigmoid activation function, and the main benefit of the ReLU activation function over the Sigmoid activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-581edc895a350827",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "The activation function transforms the input to the hidden layer into its output and allows us to approximate complex decision boundaries. The main beneift of Tanh over Sigmoid activation function is that Tanh is bound between -1 and 1 (vs. between 0 and 1 in Sigmoid).  These boundaries result in a steeper derivative and greater efficiencies. By replacing all negative values with 0, ReLU is simpler and more computationally efficient than the Sigmoid activiation function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d49d5273a50005b5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 5b: Activation Functions (5 points)\n",
    "\n",
    "Change the activation function in the hyperparameter list above to determine which activation function is most effective at this task.\n",
    "\n",
    "a) ReLU\n",
    "\n",
    "b) Sigmoid\n",
    "\n",
    "c) Tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-2c7a175daa0a705e",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "A. ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-aa7221b14bf17a26",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 6: Overfitting  (10 points)\n",
    "\n",
    "Define overfitting and explain how it can damage model training and results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-21faea75167bd4eb",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Models that are tuned too specifically to the training set are said to be overfit.  An overfit model's hyperparameters are too complex (tuned to the training data) which results in poor generalizability to future unseen data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-75a57b7d602e6ffc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 7: Early Stopping  (10 points)\n",
    "\n",
    "Outline a procedure for early stopping to prevent overfitting. Clearly describe how you’d use the training, validation, and test sets accuracy to decide when to stop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-40b7e3a1cce86696",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "In order to put early stopping into practice, I would split the training data into  a training set and a validation set.  I would train a model only on the training set and evaluate the model's performance on the validation set after a set number of epochs. As soon as the validation (test) set error is higher than it was on the previous model evaluation stop, I would halt training the model and use the parameters from that previous evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5ea01f05290cc74e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 8: Regularization  (10 points)\n",
    "\n",
    "Briefly explain a few common methods of regularization to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-e4fbdc9819855660",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "A few common methods of regularization to prevent overfitting are dropout, early stopping, and data augmentation. Dropout modifies the model itself by randomly selecting neurons to drop from the neural network (creating a simpler model) during each iteration of training. Early stopping involves stopping the iterative training of your neural network after the model stops performing on a held out validation dataset. Data augmentation involves transforming the input data (via rotaion, scaling, brightness) and adding transformed data to the training data.  This method forces the model to generalize as it is unable to overfit to the diverse training data. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
